{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should print something along the lines of '2.0.0-rc1'\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "labels = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes (10000, 28, 28, 1) (10000, 10) (60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Pre-process data\n",
    "img_rows, img_cols, channels = 28, 28, 1 # 32, 32, 3\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = x_train.reshape((-1, img_rows, img_cols, channels))\n",
    "x_test = x_test.reshape((-1, img_rows, img_cols, channels))\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Data shapes\", x_test.shape, y_test.shape, x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_000 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# l2: 0.01\n",
    "model_001 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_011 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_111 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# l2: 0.1\n",
    "model_002 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_022 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_222 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# l2: 1.0\n",
    "model_003 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_033 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.00)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_333 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),    \n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(1.0)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_000.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_001.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_011.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_111.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_002.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_022.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_222.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_003.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_033.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model_333.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_000 = model_000.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_001 = model_001.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_011 = model_011.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_111 = model_111.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_002 = model_002.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_022 = model_022.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_222 = model_222.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_003 = model_003.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_033 = model_033.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history_333 = model_333.fit(x_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 :  [0.998533308506012, 0.9990333318710327, 0.9991999864578247, 0.9991499781608582, 0.9989166855812073, 0.9987499713897705, 0.9991666674613953]\n",
      "001 :  [0.9998000264167786, 0.9983999729156494, 0.9994166493415833, 0.9991000294685364, 0.9990000128746033, 0.9987666606903076, 0.9993166923522949]\n",
      "011 :  [0.9976833462715149, 0.9988833069801331, 0.9978500008583069, 0.9976166486740112, 0.9984833598136902, 0.9980666637420654, 0.9981499910354614]\n",
      "111 :  [0.954200029373169, 0.9530500173568726, 0.9531000256538391, 0.9546166658401489, 0.9541000127792358, 0.9546999931335449, 0.9533666372299194]\n",
      "002 :  [0.9991666674613953, 0.9987666606903076, 0.9988333582878113, 0.9993000030517578, 0.9991666674613953, 0.9986333250999451, 0.9992333054542542]\n",
      "022 :  [0.9942499995231628, 0.9944000244140625, 0.994866669178009, 0.9945666790008545, 0.9949333071708679, 0.9942499995231628, 0.9948833584785461]\n",
      "222 :  [0.8672833442687988, 0.8671500086784363, 0.8678500056266785, 0.8689500093460083, 0.8682833313941956, 0.8659833073616028, 0.8676499724388123]\n",
      "003 :  [0.9981499910354614, 0.9986333250999451, 0.9984333515167236, 0.9984833598136902, 0.9983999729156494, 0.998283326625824, 0.9986500144004822]\n",
      "033 :  [0.9789166450500488, 0.9794333577156067, 0.9785500168800354, 0.9789833426475525, 0.979366660118103, 0.979116678237915, 0.9801833629608154]\n",
      "333 :  [0.11236666887998581, 0.11236666887998581, 0.11236666887998581, 0.11236666887998581, 0.11236666887998581, 0.11236666887998581, 0.11236666887998581]\n"
     ]
    }
   ],
   "source": [
    "print(\"000 : \", history_000.history['accuracy'][-7:])\n",
    "print(\"001 : \", history_001.history['accuracy'][-7:])\n",
    "print(\"011 : \", history_011.history['accuracy'][-7:])\n",
    "print(\"111 : \", history_111.history['accuracy'][-7:])\n",
    "print(\"002 : \", history_002.history['accuracy'][-7:])\n",
    "print(\"022 : \", history_022.history['accuracy'][-7:])\n",
    "print(\"222 : \", history_222.history['accuracy'][-7:])\n",
    "print(\"003 : \", history_003.history['accuracy'][-7:])\n",
    "print(\"033 : \", history_033.history['accuracy'][-7:])\n",
    "print(\"333 : \", history_333.history['accuracy'][-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_000 Base accuracy on regular images: [0.1961710900068283, 0.9793999791145325]\n",
      "Model_001 Base accuracy on regular images: [0.16401880979537964, 0.9797999858856201]\n",
      "Model_011 Base accuracy on regular images: [0.17259876430034637, 0.9700999855995178]\n",
      "Model_111 Base accuracy on regular images: [0.2834061086177826, 0.9585999846458435]\n",
      "Model_002 Base accuracy on regular images: [0.14228284358978271, 0.9793999791145325]\n",
      "Model_022 Base accuracy on regular images: [0.15833306312561035, 0.97079998254776]\n",
      "Model_222 Base accuracy on regular images: [0.7657021880149841, 0.8709999918937683]\n",
      "Model_003 Base accuracy on regular images: [0.15061841905117035, 0.9793999791145325]\n",
      "Model_033 Base accuracy on regular images: [0.19535133242607117, 0.9700999855995178]\n",
      "Model_333 Base accuracy on regular images: [2.3010876178741455, 0.11349999904632568]\n"
     ]
    }
   ],
   "source": [
    "# Assess base model accuracy on regular images\n",
    "print(\"Model_000 Base accuracy on regular images:\", model_000.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_001 Base accuracy on regular images:\", model_001.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_011 Base accuracy on regular images:\", model_011.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_111 Base accuracy on regular images:\", model_111.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_002 Base accuracy on regular images:\", model_002.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_022 Base accuracy on regular images:\", model_022.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_222 Base accuracy on regular images:\", model_222.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_003 Base accuracy on regular images:\", model_003.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_033 Base accuracy on regular images:\", model_033.evaluate(x=x_test, y=y_test, verbose=0))\n",
    "print(\"Model_333 Base accuracy on regular images:\", model_333.evaluate(x=x_test, y=y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create adversarial pattern\n",
    "def adversarial_pattern(image, label, model):\n",
    "\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        \n",
    "        label = label.reshape(1, -1)\n",
    "\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        loss = loss(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five\n",
      "three\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a signle adversarial example\n",
    "image = x_train[0]\n",
    "image_label = y_train[0]\n",
    "\n",
    "perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label, model_000).numpy()\n",
    "adversarial = image + perturbations * 0.1\n",
    "\n",
    "print(labels[model_000.predict(image.reshape((1, img_rows, img_cols, channels))).argmax()])\n",
    "print(labels[model_000.predict(adversarial).argmax()])\n",
    "\n",
    "if channels == 1:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols)))\n",
    "else:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols, channels)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adversarial data generator\n",
    "def generate_adversarials(batch_size, model):\n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        for batch in range(batch_size):\n",
    "            N = random.randint(0, 100)\n",
    "\n",
    "            label = y_train[N]\n",
    "            image = x_train[N]\n",
    "            \n",
    "            perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), label, model).numpy()\n",
    "            \n",
    "            epsilon = 0.1\n",
    "            adversarial = image + perturbations * epsilon\n",
    "            \n",
    "            x.append(adversarial)\n",
    "            y.append(y_train[N])\n",
    "        \n",
    "        \n",
    "        x = np.asarray(x).reshape((batch_size, img_rows, img_cols, channels))\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate and visualize 12 adversarial images\n",
    "# adversarials, correct_labels = next(generate_adversarials(12))\n",
    "# for adversarial, correct_label in zip(adversarials, correct_labels):\n",
    "#     print('Prediction:', labels[model.predict(adversarial.reshape((1, img_rows, img_cols, channels))).argmax()], ', Truth:', labels[correct_label.argmax()])\n",
    "#     if channels == 1:\n",
    "#         plt.imshow(adversarial.reshape(img_rows, img_cols))\n",
    "#     else:\n",
    "#         plt.imshow(adversarial)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate adversarial data\n",
    "# x_adversarial, y_adversarial = np.load(\"x_adv_10k.npy\"), np.load(\"y_adv_10k.npy\")\n",
    "x_adversarial_train, y_adversarial_train = next(generate_adversarials(3000, model_011))\n",
    "x_adversarial_test, y_adversarial_test = next(generate_adversarials(1000, model_011))\n",
    "\n",
    "x_adversarial_test2, y_adversarial_test2 = next(generate_adversarials(1000, model_011))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on adversarial images: [18.69663429260254, 0.19099999964237213]\n"
     ]
    }
   ],
   "source": [
    "# Assess base model on adversarial data\n",
    "print(\"Base accuracy on adversarial images:\", model_011.evaluate(x=x_adversarial_test, y=y_adversarial_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate adversarial data\n",
    "# x_adversarial, y_adversarial = np.load(\"x_adv_10k.npy\"), np.load(\"y_adv_10k.npy\")\n",
    "x_adversarial_train, y_adversarial_train = next(generate_adversarials(3000, model_022))\n",
    "x_adversarial_test, y_adversarial_test = next(generate_adversarials(1000, model_022))\n",
    "\n",
    "x_adversarial_test2, y_adversarial_test2 = next(generate_adversarials(1000, model_022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on adversarial images: [17.923717498779297, 0.11500000208616257]\n"
     ]
    }
   ],
   "source": [
    "# Assess base model on adversarial data\n",
    "print(\"Base accuracy on adversarial images:\", model_022.evaluate(x=x_adversarial_test, y=y_adversarial_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
