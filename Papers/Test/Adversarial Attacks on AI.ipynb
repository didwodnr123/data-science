{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for paper\n",
    "\n",
    "* Robust regularization using layer-dependent regularization control to resist adversarial attacks.\n",
    " - 유사 아이디어 자료/논문 리뷰\n",
    " - 2차원 자료 분류 문제 실험 (결정경계 시각적 확인) [관련자료](https://github.com/junji64/Machine-Learning/blob/master/Machine%20Learning%20Exercise%20%232-2%20Regularization.ipynb) : Logistic regression 에서 Neural Network 으로 변경 필요\n",
    " - 실질적 이미지 자료 실험 (예: 가위 바위 보)  유효성 검증 [관련 자료](https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7)\n",
    " - ClaverHans 에서 제공되는 Adv. Attack 및 자료 활용.\n",
    " - Adversarial Attack 시험 - 아래의 FSGM 방법부터 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적대적 공격(Adversarial Attacks) on AI\n",
    "\n",
    "적대적 공격은 기계학습 모델에 입력을 약간 수정하여 잘못된 예측을 하도록 속일 수 있는 현상입니다. 대부분의 경우 이러한 수정은 1픽셀의 색상 변경에서부터 지나치게 압축된 JPEG처럼 보이는 이미지의 극단적인 경우에 이르기까지 인간에게 눈에 띄지 않거나 중요하지 않은 것입니다. 그러나 최고 상태의 모델 조차도 이러한 변형된 입력에 매우 확신하며 기괴한 결과를 생성할 수 있음이 밝혀졌습니다.\n",
    "\n",
    "![](https://miro.medium.com/max/2132/0*pynf2nAgKkM5x_T-)\n",
    "\n",
    "이 취약점은 AI의 현재 상태에 두 가지 심각한 질문을 던집니다.\n",
    "\n",
    "1. 머신러닝 모델은 우리가 원하는 대로 우리 세상의 추상적인 아이디어와 개념적 계층 구조를 실제로 이해하고 있습니까? 아니면 예측을 하기 위해 입력의 통계적 것에 의존하고 있습니까?\n",
    "\n",
    "\n",
    "2. 과도한 탐색(exploitation)과 의도하지 않은 결과의 위험없이 생산 환경에서 모델을 안전하고 안정적으로 배포할 수 있습니까?\n",
    "\n",
    "\n",
    "Szegedy et. al (2013)[1]에 의해 발견된 적대적 공격은 기계학습 연구의 주요한 한 주제가 되었습니다. 적대적 공격의 주요한 우려되는 특징은 다음과 같습니다.\n",
    "\n",
    "* **인식불능성(Imperceptibility)** : 적은 양의 교란을 추가하거나 입력의 제한된 수의 차원을 따라 값을 약간 수정하여 적대적 예제를 효과적으로 생성 할 수 있습니다. 이러한 미묘한 수정으로 인해 인간이 감지하는 것이 거의 불가능하지만 모델은 모델이 입력을 합성하고 주의를 집중하고 의미를 학습하는 방법에 대한 우리의 이해를 어렵게 만드는 높은 확신을 가지고 잘못 분류합니다.\n",
    "\n",
    "\n",
    "* **표적된 조작(Targeted Manipulation)**: 공격 샘플은 공격자가 의도한대로 정확하게 잘못된 클래스를 출력하도록 모델을 조작하는 방식으로 생성 될 수 있습니다. 이것은 단순히 시스템을 깨뜨리는 것이 아니라 자신의 이득을 위해 시스템을 조작할 가능성을 열어줍니다.\n",
    "\n",
    "\n",
    "* **전이가능성(Transferability)**: 하나의 모델에 대해 생성된 적대적인 예제는 동일한 작업에 대해 훈련된 다른 아키텍처로도 네트워크를 속일 수 있습니다. 더욱 놀랍게도 이러한 다른 모델은 종종 일치하는 잘못된 클래스로 인식합니다. 이 속성을 통해 공격자는 대리 모델(동일한 아키텍처 또는 동일한 알고리즘 클래스 일 필요는 없음)을 근사치로 사용하여 대상 모델(오라클이라고도 함)에 대한 공격을 생성 할 수 있습니다.\n",
    "\n",
    "\n",
    "* **이론적 모델의 부족**: 현재 적대적 공격이 왜 그렇게 효과적으로 작동하는지에 대해 널리 받아 들여지는 이론적 모델은 없습니다. 선형성, 불변성 및 여러 방어 메커니즘으로 이어지는 비 강력한 특징과 같은 몇 가지 가설이 제시되었지만, 그 중 어느 것도 강력한 모델과 탄력적인 방어를 제시하는 만병 통치약 역할을 하지 못했습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 공격 능력(Adversarial abilities)\n",
    "공격자가 목표로 삼는 AI에 대한 사전 지식을 얼마나 알고 있느냐에 따라서 아래와 같이 공격 능력을 구분합니다.\n",
    "\n",
    "* **White-box**: 타겟 AI에 대해 모두 알고 있는 상황. 예를 들어, 딥러닝의 구조, Weight, Optimizer 등\n",
    "\n",
    "\n",
    "* **Black-box**: 타겟 AI의 입력과 그 출력만 확인 가능한 상황\n",
    "\n",
    "\n",
    "* **Grey-box**: White-box와 Black-box의 중간 단계\n",
    "\n",
    "\n",
    "#### 공격 목표(Adversarial goals)\n",
    "공격자의 의도에 따라서 목표를 아래와 같이 분류합니다. 단순히 틀리게만 하는 수준에서 원하는 방향으로 행동을 유도하는 것까지 다양한 난이도의 목표가 있습니다.\n",
    "\n",
    "* **Confidence reduction**: AI의 예측 신뢰도를 낮추는 방향. 예를 들어, 90%의 신뢰도를 가지는 사과 예측을 55%로 낮추는 공격\n",
    "\n",
    "\n",
    "* **Non-targeted misclassification**: 오답을 유발하는 공격\n",
    "\n",
    "\n",
    "* **Targeted misclassification**: 의도한 오답을 유발하는 공격\n",
    "\n",
    "\n",
    "* **Source-Target misclassification**: 입력에 따라서 오답을 유발하는 공격\n",
    "\n",
    "위의 공격 능력과 공격 목표에 따라서 공격 난이도가 아래의 그림과 같이 달라집니다.\n",
    "\n",
    "![the magnitude of difficulty in generating adversarial samples.](https://deepnotes.io/public/images/Screenshot_2019-07-03_11-a6f468e7-2b54-4258-87a0-a6e7d20104e9.19.58.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 공격 알고리즘\n",
    "이번 튜토리얼에서 공격 알고리즘에 대해서 다루지는 않지만 최근 몇 년간 많은 알고리즘이 발표되었습니다. FSGM, DeepFool, Universal Adversarial Pertubations 등이 있습니다.\n",
    "\n",
    "* FSGM(Fast Sign Gradient Method) \n",
    "  -  [Explaining and Harnessing Adversarial Examples [ICLR 2015]](https://arxiv.org/pdf/1412.6572.pdf) - [video-original](https://youtu.be/CIfsB_EYsVI) / [video_kr-1](https://youtu.be/TfDO2guk0ug) / [video_kr-2]\n",
    "  -\n",
    "  \n",
    "* Other\n",
    "\n",
    "#### 방어\n",
    "공격이 노이즈를 적극적으로 활용하는 것처럼 방어도 노이즈에 초점을 맞춰 이루어집니다. 대표적으로 아래와 같은 접근이 있습니다.\n",
    "\n",
    "* 공격쪽에서 생성된 이미지를 추가 학습 데이터로 활용\n",
    "* 입력이 노이즈가 추가된 이미지인지 아닌지 판별하는 AI 사용\n",
    "* Defensive Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Web documents\n",
    "    * https://deepnotes.io/adversarial-attack\n",
    "    * https://tantara.medium.com/adversarial-attack-part-1-a830ec92acde\n",
    "\n",
    "    * https://paperswithcode.com/task/adversarial-attack\n",
    "    * https://www.pyimagesearch.com/2020/10/19/adversarial-images-and-attacks-with-keras-and-tensorflow/\n",
    "    * https://blog.lgcns.com/2191?category=604440\n",
    "    \n",
    "\n",
    "* Papers (with video)\n",
    "    * [Adversarial Attacks and Defenses in Images, Graphs and Text: A Review](https://arxiv.org/pdf/1909.08072.pdf)\n",
    "    * [Explaining and Harnessing Adversarial Examples [ICLR 2015]](https://arxiv.org/pdf/1412.6572.pdf) - [video-original](https://youtu.be/CIfsB_EYsVI) / [video_kr-1](https://youtu.be/TfDO2guk0ug) / [video_kr-2](https://youtu.be/99uxhAjNwps)\n",
    "    * [PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks](https://arxiv.org/pdf/1806.00088.pdf) - [video_kr](https://youtu.be/VQsG_Yk9KuQ)\n",
    "    * [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/pdf/1608.04644.pdf) -  [video_kr](https://youtu.be/9kRWHKPyfwQ)\n",
    "    \n",
    "    \n",
    "* Codes\n",
    "    * https://www.tensorflow.org/tutorials/generative/adversarial_fgsm\n",
    "    \n",
    "    \n",
    "    \n",
    "* Videos (YouTube)\n",
    "    * https://youtu.be/VQsG_Yk9KuQ\n",
    "    * https://youtu.be/JjlV62_kGUc\n",
    "    * https://youtu.be/RYpmTldTkcw\n",
    "    * https://youtu.be/-p2il-V-0fk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
